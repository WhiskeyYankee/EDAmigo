---
title: "EDAmigo Vignette"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{EDAmigo Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```

EDAmigo is a package designed to make Exploratory Data Analysis (EDA) simple. It provides users with streamlined options for automatically cleaning, processing, and viewing their data. It also supports interactive EDA methods which experienced users may find useful. The following demonstration shows how EDAmigo can be used to support the EDA process:


# Step 1: Load package & Import Data

The first step in the EDA process is to import a dataset and load the EDAmigo library. The package comes with two example datasets: Fires and Finance. We will use the fires data for this example.

```{r setup}
library(EDAmigo)

# Define the dataframe
df <- fires
```


# Step 2: Manually Set Date Fields
Dates can be stored in a wide variety of ways and automatically processing all possible date storage types is outside of the scope of the EDAmigo package. EDAmigo requires all dates and times to be properly classed as 'Date', 'POSIXlt', or 'POSIXct.' If your dataset contains dates, you will need to set manually set them to 'Date', 'POSIXlt', or 'POSIXct' prior to proceeding. Let's check the classes of the columns of the fires dataframe using the str() function:

```{r}
str(df)

```

All of our date columns are type 'Date.' However, there are several special characters, improperly typed columns, and missing values. These can be handled using a variety of methods in EDAmigo.

# Step 3: Clean Data
The fires data set is an example of raw data that 

We can either use the autoClean() function to handle special characters, improperly typed columns, and missing values at the same time or handle each separately using handleSpecial(), detectTypes(), and handleMissing() individually. Let's use the individual functions to provide more in depth detail about the abilities of each function.

```{r}
# Store output for future use.
# Pass in:  
  # the dataframe, 
  # a factor tolerance of 10% to automatically coerce values with 1% or less unique values to factors
  # a user tolerance of 10%, this will prompt the user for direction when a column has <= 10% unique values
  # by setting factor_tol and type_user_tol to the same level, we will not receieve any user interactivity

remove_special <- detectTypes(df, factor_tol = 10, type_user_tol = 10)

```

autoClean() provides users with the ability to interact with the EDA process. In the example provided, interaction is turned off to allow the vignette to knit properly. However, videos will be included throughout this vignette to provide you with demonstrations of this interactivity.

<video width="840" height="630" autoplay loop muted>
  <source src="C:\Users\moore\Videos\Captures\EDAmigo - main - RStudio 2023-12-02 20-40-04.mp4" type="video/mp4">
</video>



In our example we are storing the autoClean() output as 'result.' Let's examine the output of autoClean() function. The function returned a list of 6 elements: clean_df, 


## clean_df
This is the resulting dataframe with special characters removed, columns coerced to an appropriate class, and missing values handled. Compared to our original dataframe, we see that 5 columns were removed and that many columns have been coerced to a different class. 
```{r}
# Store our clean dataframe for future use
cleaned_fires <- result$clean_df
str(cleaned_fires)

```

Now, let's look at the remaining output, which are derived from other functions.

## handleSpecial() output
This shows the special characters that were found, versus the special characters that were actually removed.
```{r}
# Examine results of the handleSpecial() function
result$special_found_replaced
```

## detectTypes() output
This output is a record of what the detectTypes() function changed. The first column indicates the % of unique values contained within the named column of the original dataframe. Next, we see the original column class, and the resulting column class.
```{r}
# Examine the results of the detectTypes() function
result$type_stats
```

## handleMissing() output
Lastly, the handleMissing output includes information about which columns are dropped, and at what point in the process. 'missing_stats' includes the percent of missing values for each column throughout each step in the process. It is important to note that this output may change, depending upon the options the user inputs and/or selects during interactive sections of the function.

The final list of dropped columns and dropped rows are also provided, so the user knows which rows and columns are no longer contained in the final dataframe.
```{r}
# Examine the results of the handleMissing() function
result$missing_stats
result$dropped_cols
result$dropped_rows
```


## boxCox
Often times it is desirable to use a power transformation to normalize data and/or reign outliers to improve downstream model performance.  This can be done using box cox transformations on each numeric column and evaluating the results to see if a transformation would be helpful. The boxCox function differs from implementations in other packages in that it performs transformations on multiple columns at the same time and offers more output to help aid the EDA process. When the FILTER parameter is set to TRUE, the function only outputs results for numeric columns where using a power transformation makes the data more normal and reduces outliers. The transformation are sorted to indicate which ones do the best at achieving normality so the user can prioritize which ones to review and potentially use. 

Lets use boxCox to evaluate potential transformations on the cleaned fire data. The autoClean function results in data set that has 4 numeric columns and 1 integer column. To illustrate how the boxCox function can ease the EDA process, lets evaluate the data first using FLITER = FALSE

```{r}
# Clean Data using autoClean function
fires_cleaned = autoClean(fires, special_user_level = 0, factor_tol = 10, type_user_tol = 10
                    , drop_user_level = 0, impute_user_level = 0, impute_factors = TRUE)

# Run the boxCox function
fires_boxCox_Trans = boxCox(fires_cleaned$clean_df , FILTER = FALSE)

# Evaluate the results
fires_boxCox_Trans$boxCox_Results
```

In the results above we get a warning that one or more of the variables did not converge in the specified lambda_1 range. Looking at the function output we see that the Lat variable does not have a lower bound and that the selected lambda_1 value is equal to -3. This is the variable that did not converge in the specified range. We can also see that the Anderson Darling statistic for the transformation Lat variable did not change by much. In fact, as we expand the lambda range, the Anderson Darling statistic for Latitude doesn't improve by more than a few points. This is a variable that doesn't benefit much from a power transformation. 

We also observe in the function output that while the LocalIncidentIdentifier becomes more normal with a power transformation, it does so at the cost of increasing the number of number of outliers in the transformed data. Here we consider any values that fall outside the whiskers of a box plot to be outliers. This is indication that we might not want to transform this value either. 

In fact, the only variables that have a converging lambda within the specified range, are more normal in shape after the transformation, and don't have an increase in the number of outliers as a result of the transformation are the IncidentSize and InitialResponseAcres variables. Lets see how the boxCox function works when FILTER is set to true: 

```{r}
# Run the boxCox function
fires_boxCox_Trans = boxCox(fires_cleaned$clean_df , FILTER = TRUE)

# Evaluate the results
fires_boxCox_Trans$boxCox_Results
```

This time, we see that only the IncidentSize and InitialResponseAcres variables are kept in the output. The warning about convergence is only with regards to the Lat variable and does let us know that we may consider increasing the range and we could do that be looking at the unfiltered data to get an idea of what isn't converging.  Our trimmed down results make it easier to determine which transformations we might consider. 


